{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74b06877-ac3c-416f-8822-5b25c2b46d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm, trange\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle5 as pickle\n",
    "import plotly.express as px\n",
    "import itertools\n",
    "import argparse\n",
    "import scipy.stats\n",
    "import scipy.special as special\n",
    "from typing import Dict, List, Any, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f7c76e7-36f3-4ff4-bcd6-5a785f81b326",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed: int):\n",
    "    import random, os\n",
    "    import numpy as np\n",
    "    \n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd0ded5b-4ced-4360-83e3-71ab5c376772",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pickle(file_path: str) -> Any:\n",
    "\twith open(file_path, \"rb\") as handle:\n",
    "\t\treturn pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "847ec7e2-db6b-4647-ae9f-aed719ea5247",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_pickle(file: Any, file_path: str) -> None:\n",
    "    with open(file_path, 'wb') as handle:\n",
    "        pickle.dump(file, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fca42a9d-283b-4386-8dcb-066a7c87a551",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_freqs(i2s):\n",
    "    in_v = Counter()\n",
    "    out_v = Counter()\n",
    "    \n",
    "    for txt in i2s[\"In\"]:\n",
    "        tokens = txt.split()\n",
    "        in_v.update(tokens)\n",
    "\n",
    "    for txt in i2s[\"Out\"]:\n",
    "        tokens = txt.split()\n",
    "        out_v.update(tokens)\n",
    "    \n",
    "    total = sum(in_v.values())\n",
    "    for k in in_v:\n",
    "        in_v[k] /= total\n",
    "\n",
    "    total = sum(out_v.values())\n",
    "    for k in out_v:\n",
    "        out_v[k] /= total\n",
    "        \n",
    "    return in_v, out_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c29dca1a-bdbb-47ed-9ffb-32083a07f86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rarity(in_txt, out_txt, in_v, out_v):\n",
    "    in_toks = in_txt.split()\n",
    "    out_toks = out_txt.split()\n",
    "    \n",
    "    in_rarity, out_rarity = 0, 0\n",
    "    in_len, out_len = len(in_toks), len(out_toks)\n",
    "    \n",
    "    for tok in in_toks:\n",
    "        in_rarity += in_v[tok]\n",
    "        \n",
    "    in_rarity /= in_len\n",
    "    \n",
    "    for tok in out_toks:\n",
    "        out_rarity += out_v[tok]\n",
    "    \n",
    "    out_rarity /= out_len\n",
    "    \n",
    "    return -np.log(in_rarity), -np.log(out_rarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc03c329-6f3e-40c9-96b5-5f3fda086106",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "STRING_TRUNCATE = 50\n",
    "\n",
    "def get_scores(dir_path: str, converge_epoch: int, string_truncate: int, min_epoch: int = 3) -> Tuple[Dict[int, Dict[str, List[float]]], Dict[str, List[Any]]]:\n",
    "    file_list = os.listdir(dir_path)\n",
    "    idx_to_sentences: Dict[int, Dict[str, str]] = read_pickle(os.path.join(dir_path, \"idx_to_sentences.pickle\"))\n",
    "\n",
    "    file_list = [f for f in file_list if f[:5] == \"epoch\"]\n",
    "    file_list = [f for f in file_list if int(f.split(\"_\")[0].replace(\"epoch\", \"\")) > min_epoch and int(f.split(\"_\")[0].replace(\"epoch\", \"\")) < converge_epoch]\n",
    "    file_list = sorted(file_list, key= lambda s: int(s.split(\"_\")[1].replace(\"stepidx\", \"\")))\n",
    "\n",
    "    # print(\"Loading files in:\", dir_path)\n",
    "    idxs, ppls, chias, bleus = [], [], [], []\n",
    "    for file_name in file_list:\n",
    "        file_path = f\"{dir_path}/{file_name}\"\n",
    "        # print(file_name)\n",
    "        if \"ppl\" in file_path:\n",
    "            ppls.extend(read_pickle(file_path).tolist())\n",
    "        elif \"chia\" in file_path:\n",
    "            chias.extend(read_pickle(file_path).tolist())\n",
    "        elif \"bleu\" in file_path:\n",
    "            bleus.extend(read_pickle(file_path))\n",
    "        elif \"idx\" in file_path:\n",
    "            idxs.extend(read_pickle(file_path).tolist())\n",
    "        else:\n",
    "            output_csv_name = file_path\n",
    "\n",
    "    items = list(zip(idxs, ppls, chias, bleus))\n",
    "    items = sorted(items, key=lambda i: i[0])\n",
    "    idx_dict: Dict[int, Dict[str, List[float]]] = {}\n",
    "    for item in items:\n",
    "        if item[0] not in idx_dict:\n",
    "            idx_dict[item[0]] = {\"inv_ppl\": [1 / item[1]], \"chia\": [item[2]], \"bleu\": [item[3]]}\n",
    "        else:\n",
    "            idx_dict[item[0]][\"inv_ppl\"].append(1 / item[1])\n",
    "            idx_dict[item[0]][\"chia\"].append(item[2])\n",
    "            idx_dict[item[0]][\"bleu\"].append(item[3])\n",
    "\n",
    "    i2s = {\"Index\": [], \"In\": [], \"Out\": [], \"In abbv.\": [], \"Out abbv.\": [], \"In Len\": [], \"Out Len\": [], \"In Rarity\": [], \"Out Rarity\": []}\n",
    "\n",
    "    for k, v in idx_to_sentences.items():\n",
    "        i2s[\"Index\"].append(k)\n",
    "        i2s[\"In\"].append(v[\"in\"])\n",
    "        i2s[\"Out\"].append(v[\"out\"])\n",
    "        i2s[\"In abbv.\"].append(v[\"in\"][:STRING_TRUNCATE])\n",
    "        i2s[\"Out abbv.\"].append(v[\"out\"][:STRING_TRUNCATE])\n",
    "        i2s[\"In Len\"].append(len(v[\"in\"].split()))\n",
    "        i2s[\"Out Len\"].append(len(v[\"out\"].split()))\n",
    "\n",
    "    in_v, out_v = get_word_freqs(i2s)\n",
    "    for k, v in idx_to_sentences.items():\n",
    "        in_rarity, out_rarity = get_rarity(v[\"in\"], v[\"out\"], in_v, out_v)\n",
    "        i2s[\"In Rarity\"].append(in_rarity)\n",
    "        i2s[\"Out Rarity\"].append(out_rarity)\n",
    "\n",
    "    return idx_dict, i2s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50fff27f-57cc-4a75-b76e-0efe37123d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def create_vocab(df):\n",
    "\tin_v = Counter()\n",
    "\tout_v = Counter()\n",
    "    \n",
    "\tfor idx, txt in df[\"In\"].items():\n",
    "\t\ttokens = txt.split()\n",
    "\t\tin_v.update(tokens)\n",
    "         \n",
    "\tfor idx, txt in df[\"Out\"].items():\n",
    "\t\ttokens = txt.split()\n",
    "\t\tout_v.update(tokens)\n",
    "\n",
    "\treturn set(in_v.keys()), set(out_v.keys()), in_v, out_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bde8fd88-8e0c-4848-83c8-440704b82bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_statistics(epoch: int, idx_dict: Dict[int, Dict[str, List[float]]], i2s: Dict[str, List[Any]]) -> pd.DataFrame:\n",
    "\tidx_mean_var_dict: Dict[int, Dict[str, Tuple[float, float]]] = {}\n",
    "\tidx_mean_var_list: List[Tuple[int, float, float, float, float, float, float, float, float]] = []\n",
    "\tscore_names = [\"inv_ppl\", \"chia\", \"bleu\"]\n",
    "\tfor idx, scores in idx_dict.items():\n",
    "\t\tscores_list = []\n",
    "\t\tfor score_name in score_names:\n",
    "\t\t\tscore_arr = np.array(scores[score_name][:epoch])\n",
    "\t\t\tmean = score_arr.mean()\n",
    "\t\t\tvar = score_arr.var()\n",
    "\t\t\tscores_list.extend([mean, var])\n",
    "\t\t\n",
    "\t\tidx_mean_var_list.append(tuple((idx, *scores_list)))\n",
    "\n",
    "\ti2s_df = pd.DataFrame.from_dict(i2s)\n",
    "\n",
    "\n",
    "\tdf = pd.DataFrame(idx_mean_var_list, columns =['Index', 'Confidence - Inverse PPL', 'Variability - Inverse PPL', \\\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t'Confidence - CHIA', 'Variability - CHIA', \\\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t'Confidence - BLEU', 'Variability - BLEU'])\n",
    "\n",
    "\tcartography = pd.merge(df, i2s_df, on=\"Index\")\n",
    "\n",
    "\treturn cartography"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9e43912-8561-4067-9c89-b271bbe079bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_scores(dir_path: str, plot_path: str, converge_epoch: int) -> None:\n",
    "\tidx_dict = get_scores(dir_path, plot_path, converge_epoch)\n",
    "\t\n",
    "\tfor epoch in trange(3, converge_epoch, 2):\n",
    "\t\tdf = calculate_statistics(epoch, idx_dict)\n",
    "\n",
    "\t\tplot_types = [\"inv_ppl\", \"chia\", \"bleu\"]\n",
    "\n",
    "\t\tfor plot_type in tqdm(plot_types, \"Plots\"):\n",
    "\t\t\tplot(df, plot_path, str(epoch), plot_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c29c2fb0-7401-4cf7-b57c-8451ece5082c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_subset(subset_df: pd.DataFrame, ds_name: str, subset_fname: str) -> None:\n",
    "    subset_idx = subset_df[\"Index\"].tolist()\n",
    "    subset_idx = [int(i) for i in subset_idx]\n",
    "    subset_idx = set(subset_idx)\n",
    "    \n",
    "    os.makedirs(os.path.join(\"subsets\", ds_name), exist_ok=True)\n",
    "    write_pickle(subset_idx, os.path.join(\"subsets\", ds_name, subset_fname))\n",
    "    print(f\"subset_idx: {len(subset_idx)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "32f2ab5c-4900-4104-884e-dc926d1141e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "def choose_subset(df: pd.DataFrame, metric: str, criteria: str, ds_name: str, subset_fname:str, ratio:float = 0.33, write=True) -> pd.DataFrame:\n",
    "    assert metric in [\"Inverse PPL\", \"Neg PPL\", \"CHIA\", \"BLEU\"]\n",
    "    assert criteria in [\"Easy to Learn\", \"Ambiguous\", \"Hard to Learn\", \"Random\"]\n",
    "    \n",
    "    if criteria == \"Easy to Learn\":\n",
    "        sort_by = f\"Confidence - {metric}\"\n",
    "        ascending = False\n",
    "    elif criteria == \"Ambiguous\":\n",
    "        sort_by = f\"Variability - {metric}\"\n",
    "        ascending = False\n",
    "    elif criteria == \"Hard to Learn\":\n",
    "        sort_by = f\"Confidence - {metric}\"\n",
    "        ascending = True\n",
    "        \n",
    "    if criteria == \"Random\":\n",
    "        sorted_df = df.sample(frac=1)\n",
    "    else:\n",
    "        sorted_df = df.sort_values(by=[sort_by], ascending=ascending)\n",
    "\n",
    "    sorted_df = sorted_df.reset_index(drop=True)\n",
    "    subset_df = sorted_df.iloc[:int(len(df)*ratio),:]\n",
    "    \n",
    "    subset_idx = subset_df[\"Index\"].tolist()\n",
    "    subset_idx = [int(i) for i in subset_idx]\n",
    "    subset_idx = set(subset_idx)\n",
    "    print(\"start subset\", len(subset_idx))\n",
    "    \n",
    "    all_in_v, all_out_v, _, _ = create_vocab(df)\n",
    "    subset_in_v, subset_out_v, subset_in_v_counts, subset_out_v_counts = create_vocab(subset_df)\n",
    "\n",
    "    add_ex_i = []\n",
    "    remove_ex_i = []\n",
    "    \n",
    "    for i in trange(int(len(df)*ratio), len(df)):\n",
    "        new_in, new_out = sorted_df.iloc[i, 7], sorted_df.iloc[i, 8]\n",
    "        new_in_tokens, new_out_tokens = set(new_in.split()), set(new_out.split())\n",
    "        \n",
    "        if (new_in_tokens - subset_in_v) or (new_out_tokens - subset_out_v):\n",
    "            # print(f\"In vocab dif: {(new_in_tokens - subset_in_v)}\")\n",
    "            # print(f\"Out vocab dif: {(new_out_tokens - subset_out_v)}\")\n",
    "            add_ex_i.append(i)\n",
    "            subset_in_v = subset_in_v.union(new_in_tokens)\n",
    "            subset_out_v = subset_out_v.union(new_out_tokens)\n",
    "            subset_in_v_counts.update(new_in.split())\n",
    "            subset_out_v_counts.update(new_out.split())\n",
    "            \n",
    "    in_counter = subset_in_v_counts\n",
    "    out_counter = subset_out_v_counts\n",
    "    \n",
    "    removed_amount = 0\n",
    "    \n",
    "    for i in trange(0, int(len(df)*ratio)):\n",
    "        print(len(remove_ex_i), len(add_ex_i))\n",
    "        if len(remove_ex_i) == len(add_ex_i):\n",
    "            break\n",
    "            \n",
    "        ex_in, ex_out = sorted_df.iloc[i, 7], sorted_df.iloc[i, 8]\n",
    "        ex_in_counter, ex_out_counter = Counter(ex_in.split()), Counter(ex_out.split())\n",
    "        \n",
    "        upd_in_counter = in_counter - ex_in_counter\n",
    "        upd_out_counter = out_counter - ex_out_counter\n",
    "        \n",
    "        ex_in_words, ex_out_words = list(set(ex_in.split())), list(set(ex_out.split()))\n",
    "        \n",
    "        remove = True\n",
    "        for word in ex_in_words:\n",
    "            if upd_in_counter[word] <= 1:\n",
    "                remove = False\n",
    "        \n",
    "        for word in ex_out_words:\n",
    "            if upd_out_counter[word] <= 1:\n",
    "                remove = False\n",
    "                \n",
    "        if remove:\n",
    "            in_counter = upd_in_counter\n",
    "            out_counter = upd_out_counter\n",
    "            remove_ex_i.append(i)\n",
    "            \n",
    "    subset_df = pd.concat([subset_df, df.iloc[add_ex_i]])\n",
    "    subset_df = subset_df.drop(remove_ex_i, axis=0)\n",
    "    subset_df = subset_df.reset_index(drop=True)\n",
    "    \n",
    "    assert all_in_v == set(in_counter.keys()), \"The process is wrong\"\n",
    "    assert all_out_v == set(out_counter.keys()), \"The process is wrong 2\"\n",
    "    \n",
    "    if write:\n",
    "        save_subset(subset_df, ds_name, subset_fname)\n",
    "    \n",
    "    print(len(remove_ex_i), len(add_ex_i))\n",
    "    \n",
    "    return subset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "72b5126d-f2fa-4904-9585-ff9e03ecc01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_subsets(df: pd.DataFrame, subset_dfs: List[pd.DataFrame], ds_name: str, subset_fname: str) -> pd.DataFrame:\n",
    "    \n",
    "    combined_set = pd.concat(subset_dfs)\n",
    "    combined_set = combined_set.drop_duplicates(keep=\"first\")\n",
    "    \n",
    "    if len(combined_set) > (len(df) / 2):\n",
    "        combined_set = combined_set.iloc[:int(len(df) / 2)]\n",
    "    else:\n",
    "        count = 0\n",
    "        while len(combined_set) < (len(df) / 2):\n",
    "            example = df.sample(n=1)\n",
    "            if not example.iloc[0][\"In\"] in combined_set['In'].tolist():\n",
    "                combined_set = combined_set.append(example)\n",
    "                \n",
    "    save_subset(combined_set, ds_name, subset_fname)\n",
    "    \n",
    "    return combined_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e2075ae2-2739-4421-97be-73ff56a4ebc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(df, plot_type=\"inv_ppl\", color_column=\"_merge\"):\n",
    "\tif plot_type == \"inv_ppl\":\n",
    "            # print(df[\"_merge\"].unique())\n",
    "            #df[\"_merge\"] = df[\"_merge\"].cat.remove_categories(\"right_only\")\n",
    "            # print(df[\"_merge\"].unique())\n",
    "            #assert '_merge' in df.columns, \"_merge not in columns\"\n",
    "            fig = px.scatter(df, x=\"Variability - Inverse PPL\", y=\"Confidence - Inverse PPL\", custom_data=['In abbv.', 'Out abbv.', 'In Len', 'Out Len'], color=color_column) # , range_color=[0,1]\n",
    "            fig.update_layout(yaxis_range=[0, 1])\n",
    "            fig.update_traces(\n",
    "                hovertemplate=\"<br>\".join([\n",
    "                    \"Variability - Inverse PPL: %{x}\",\n",
    "                    \"Confidence - Inverse PPL: %{y}\",\n",
    "                    \"In: %{customdata[0]}\",\n",
    "                    \"Out: %{customdata[1]}\",\n",
    "                    \"In Len: %{customdata[2]}\",\n",
    "                    \"Out Len: %{customdata[3]}\", \n",
    "                ])\n",
    "            )\n",
    "\telif plot_type == \"chia\":\n",
    "\t\tfig = px.scatter(df, x=\"Variability - CHIA\", y=\"Confidence - CHIA\", custom_data=['In abbv.', 'Out abbv.', 'In Len', 'Out Len'], color='Confidence - BLEU', range_color=[0,1])\n",
    "\t\tfig.update_layout(yaxis_range=[0, 1])\n",
    "\t\tfig.update_traces(\n",
    "\t\t\thovertemplate=\"<br>\".join([\n",
    "\t\t\t\t\"Variability - CHIA: %{x}\",\n",
    "\t\t\t\t\"Confidence - CHIA: %{y}\",\n",
    "\t\t\t\t\"In: %{customdata[0]}\",\n",
    "\t\t\t\t\"Out: %{customdata[1]}\",\n",
    "                \"In Len: %{customdata[2]}\",\n",
    "                \"Out Len: %{customdata[3]}\", \n",
    "\t\t\t])\n",
    "\t\t)\n",
    "\telif plot_type == \"bleu\":\n",
    "\t\tfig = px.scatter(df, x=\"Variability - BLEU\", y=\"Confidence - BLEU\", custom_data=['In abbv.', 'Out abbv.', 'In Len', 'Out Len'], color='Confidence - BLEU', range_color=[0,1])\n",
    "\t\tfig.update_layout(yaxis_range=[0, 1])\n",
    "\t\tfig.update_traces(\n",
    "\t\t\thovertemplate=\"<br>\".join([\n",
    "\t\t\t\t\"Variability - BLEU: %{x}\",\n",
    "\t\t\t\t\"Confidence - BLEU: %{y}\",\n",
    "\t\t\t\t\"In: %{customdata[0]}\",\n",
    "\t\t\t\t\"Out: %{customdata[1]}\",\n",
    "                \"In Len: %{customdata[2]}\",\n",
    "                \"Out Len: %{customdata[3]}\", \n",
    "\t\t\t])\n",
    "\t\t)\t\n",
    "\tfig.update_traces(marker=dict(size=3), selector=dict(mode='markers'))\n",
    "\tfig.update_layout(\n",
    "\t\tautosize=False,\n",
    "\t\twidth=800,\n",
    "\t\theight=900\n",
    "\t)\n",
    "\tfig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0386cacd-8d81-4d94-aa31-dcf8606f950e",
   "metadata": {},
   "outputs": [],
   "source": [
    "STRING_TRUNCATE = 120\n",
    "\n",
    "mtrc2abv = {\"Inverse PPL\": \"inv_ppl\", \"Neg PPL\": \"neg_ppl\", \"CHIA\": \"chia\", \"BLEU\": \"bleu\"}\n",
    "crit2abv = {\"Easy to Learn\": \"easy_to_learn\", \"Ambiguous\": \"ambiguous\", \"Hard to Learn\": \"hard_to_learn\", \"Random\": \"random\"}\n",
    "create_fname = lambda m, cr, c_e: f\"{mtrc2abv[m]}_{crit2abv[cr]}_{c_e}.pickle\"\n",
    "create_ratio_fname = lambda m, cr, c_e, rto: f\"{mtrc2abv[m]}_{crit2abv[cr]}_{c_e}_{rto}.pickle\"\n",
    "create_comb_fname = lambda m, cr1, cr2, c_e: f\"{mtrc2abv[m]}_{crit2abv[cr1]}_{crit2abv[cr2]}_{c_e}.pickle\"\n",
    "outputs_path = lambda x: f\"../scores/{x}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fe29838d-8eea-4f85-bce1-fac2f52e1f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# i2s = read_pickle(os.path.join(outputs_path(\"cfq\"), \"idx_to_sentences.pickle\"))\n",
    "# i2s_htl = read_pickle(os.path.join(outputs_path(\"cfq\"), \"idx_to_sentences_htl_20.pickle\"))\n",
    "# len(i2s), len(i2s_htl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa0a662-4972-40a8-881e-6deea061a751",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAMES = [\"0/cogs\", \"0/cfq\"]\n",
    "METRICS = [\"Inverse PPL\", \"CHIA\", \"BLEU\"]\n",
    "CRITERIA = [\"Easy to Learn\", \"Ambiguous\", \"Hard to Learn\", \"Random\"]\n",
    "CONVERGE_EPOCHS = [10, 20]\n",
    "RATIOS = [0.33]\n",
    "\n",
    "for DATASET_NAME, CONVERGE_EPOCH in zip(DATASET_NAMES, CONVERGE_EPOCHS):\n",
    "    OUTPUTS_PATH = outputs_path(DATASET_NAME)\n",
    "    idx_dict, i2s = get_scores(OUTPUTS_PATH, CONVERGE_EPOCH, STRING_TRUNCATE)\n",
    "    df = calculate_statistics(CONVERGE_EPOCH, idx_dict, i2s)\n",
    "    for METRIC in METRICS:\n",
    "        for CRITERION in CRITERIA:\n",
    "            for RATIO in RATIOS:\n",
    "                idx_fname = create_ratio_fname(METRIC, CRITERION, CONVERGE_EPOCH, RATIO)\n",
    "                subset_df = choose_subset(df, METRIC, CRITERION, DATASET_NAME, idx_fname, ratio=RATIO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ac8410e2-a388-4ffa-be23-3b6cb96ff8ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7866\n",
      "7920\n",
      "7971\n",
      "7942\n",
      "7888\n",
      "7962\n",
      "7970\n",
      "7942\n",
      "7842\n",
      "7964\n",
      "7971\n",
      "7943\n",
      "31591\n",
      "31591\n",
      "31595\n",
      "31595\n",
      "31591\n",
      "31593\n",
      "31595\n",
      "31594\n",
      "31590\n",
      "31592\n",
      "31595\n",
      "31594\n"
     ]
    }
   ],
   "source": [
    "DATASET_NAMES = [\"0/cogs\", \"0/cfq\"]\n",
    "METRICS = [\"Inverse PPL\", \"CHIA\", \"BLEU\"]\n",
    "CRITERIA = [\"Easy to Learn\", \"Ambiguous\", \"Hard to Learn\", \"Random\"]\n",
    "CONVERGE_EPOCHS = [10, 20]\n",
    "RATIOS = [0.33]\n",
    "\n",
    "for DATASET_NAME, CONVERGE_EPOCH in zip(DATASET_NAMES, CONVERGE_EPOCHS):\n",
    "    OUTPUTS_PATH = outputs_path(DATASET_NAME)\n",
    "    for METRIC in METRICS:\n",
    "        for CRITERION in CRITERIA:\n",
    "            for RATIO in RATIOS:\n",
    "                idx_fname = create_ratio_fname(METRIC, CRITERION, CONVERGE_EPOCH, RATIO)\n",
    "                subset_df = read_pickle(\"subsets/\" + DATASET_NAME + \"/\" + idx_fname)\n",
    "                print(len(subset_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d53602d6-97c0-4053-924a-ee028969156b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inv_ppl_easy_to_learn_10_0.5.pickle 11938\n",
      "inv_ppl_ambiguous_10_0.5.pickle 12026\n",
      "inv_ppl_hard_to_learn_10_0.5.pickle 12077\n",
      "chia_easy_to_learn_10_0.5.pickle 11961\n",
      "chia_ambiguous_10_0.5.pickle 12072\n",
      "chia_hard_to_learn_10_0.5.pickle 12077\n"
     ]
    }
   ],
   "source": [
    "DATASET_NAME = \"42/cogs\"\n",
    "OUTPUTS_PATH = outputs_path(DATASET_NAME)\n",
    "METRICS = [\"Inverse PPL\", \"CHIA\"] # \"BLEU\" \n",
    "CRITERIA = [\"Easy to Learn\", \"Ambiguous\", \"Hard to Learn\"] # , \"Random\" \n",
    "CONVERGE_EPOCH = 10\n",
    "RATIOS = [0.5] # 0.33, \n",
    "\n",
    "for METRIC in METRICS:\n",
    "    for CRITERION in CRITERIA:\n",
    "        for RATIO in RATIOS:\n",
    "            idx_fname = create_ratio_fname(METRIC, CRITERION, CONVERGE_EPOCH, RATIO)\n",
    "            print(idx_fname, len(list(read_pickle(os.path.join(\"subsets\", DATASET_NAME, idx_fname)))))\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e2acb2-6ecc-48c3-a67d-882399304532",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78fb5ff-7bef-4f6b-aa8b-8e867da6b976",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = \"cogs\"\n",
    "OUTPUTS_PATH = outputs_path(DATASET_NAME)\n",
    "\n",
    "METRICS = [\"Inverse PPL\"]#, \"CHIA\"]\n",
    "CRITERIA = [\"Hard to Learn\", \"Ambiguous\", \"Easy to Learn\"] #, \"Ambiguous\",  \"Random\"]\n",
    "COMBINED_CRITERIA = list(itertools.combinations([\"Hard to Learn\", \"Ambiguous\", \"Easy to Learn\"], 2))\n",
    "RATIOS = [0.5]\n",
    "CONVERGE_EPOCHS = [10]\n",
    "\n",
    "for RATIO in RATIOS:\n",
    "    for CONVERGE_EPOCH in CONVERGE_EPOCHS:\n",
    "        idx_dict, i2s = get_scores(OUTPUTS_PATH, CONVERGE_EPOCH, STRING_TRUNCATE)\n",
    "        df = calculate_statistics(CONVERGE_EPOCH, idx_dict, i2s)\n",
    "        for METRIC in METRICS:\n",
    "            merge_dfs = []\n",
    "            for CRITERION in CRITERIA:\n",
    "                idx_fname = create_ratio_fname(METRIC, CRITERION, CONVERGE_EPOCH, RATIO)\n",
    "                subset_df = choose_subset(df, METRIC, CRITERION, DATASET_NAME, idx_fname, ratio=RATIO)\n",
    "                merge_df = pd.merge(df, subset_df, on=[\"Index\", \"In\", \"Out\", \"In abbv.\", \"Out abbv.\", \"In Len\", \"Out Len\", \"In Rarity\", \"Out Rarity\", \\\n",
    "                                                       'Confidence - Inverse PPL', 'Variability - Inverse PPL', \\\n",
    "                                                        'Confidence - CHIA', 'Variability - CHIA', \\\n",
    "                                                        'Confidence - BLEU', 'Variability - BLEU'], indicator=f\"merge_{crit2abv[CRITERION]}\", how='outer')\n",
    "                merge_dfs.append(merge_df)\n",
    "\n",
    "            merge_df = merge_dfs[0]\n",
    "            for i in range(1, len(merge_dfs)):\n",
    "                merge_df = pd.merge(merge_df, merge_dfs[i], on=[\"Index\", \"In\", \"Out\", \"In abbv.\", \"Out abbv.\", \"In Len\", \"Out Len\", \"In Rarity\", \"Out Rarity\", \\\n",
    "                                                        'Confidence - Inverse PPL', 'Variability - Inverse PPL', \\\n",
    "                                                        'Confidence - CHIA', 'Variability - CHIA', \\\n",
    "                                                        'Confidence - BLEU', 'Variability - BLEU'], how='outer')\n",
    "                print(merge_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883ded4f-6ad1-45c3-823e-0609df0ac4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = \"cogs\"\n",
    "OUTPUTS_PATH = outputs_path(DATASET_NAME)\n",
    "METRIC = \"CHIA\"\n",
    "CRITERIA = \"Hard to Learn\"\n",
    "CONVERGE_EPOCH = 10\n",
    "\n",
    "idx_dict, i2s = get_scores(OUTPUTS_PATH, CONVERGE_EPOCH, STRING_TRUNCATE)\n",
    "df = calculate_statistics(CONVERGE_EPOCH, idx_dict, i2s)\n",
    "idx_fname = create_fname(METRIC, CRITERIA, CONVERGE_EPOCH)\n",
    "subset_df = choose_subset(df, METRIC, CRITERIA, DATASET_NAME, idx_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0597be64-61b1-487e-9878-d8804aff2d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = \"42/cogs\"\n",
    "OUTPUTS_PATH = outputs_path(DATASET_NAME)\n",
    "METRICS = [\"Inverse PPL\"] #[\"CHIA\", \"BLEU\"]\n",
    "CRITERIA = [\"Hard to Learn\", \"Ambiguous\", \"Easy to Learn\", \"Random\"]\n",
    "COMBINED_CRITERIA = list(itertools.combinations([\"Hard to Learn\", \"Ambiguous\", \"Easy to Learn\"], 2))\n",
    "RATIOS = [0.5]\n",
    "CONVERGE_EPOCHS = [16]\n",
    "\n",
    "for RATIO in RATIOS:\n",
    "    for CONVERGE_EPOCH in CONVERGE_EPOCHS:\n",
    "        idx_dict, i2s = get_scores(OUTPUTS_PATH, CONVERGE_EPOCH, STRING_TRUNCATE, min_epoch=6)\n",
    "        df = calculate_statistics(CONVERGE_EPOCH, idx_dict, i2s)\n",
    "        for METRIC in METRICS:\n",
    "            for CRITERION in CRITERIA:\n",
    "                idx_fname = create_ratio_fname(METRIC, CRITERION, CONVERGE_EPOCH, RATIO)\n",
    "                subset_df = choose_subset(df, METRIC, CRITERION, DATASET_NAME, idx_fname, ratio=RATIO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695afffb-e198-4d06-9ecf-5ad749509664",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b9d900-82eb-4167-bb69-acc1b3da5690",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a91df2e-2ecb-4d10-a9da-d12b559badf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = \"cfq\"\n",
    "OUTPUTS_PATH = outputs_path(DATASET_NAME)\n",
    "\n",
    "METRICS = [\"Inverse PPL\"]#, \"CHIA\"]\n",
    "CRITERIA = [\"Hard to Learn\", \"Ambiguous\", \"Easy to Learn\"] #, \"Ambiguous\",  \"Random\"]\n",
    "COMBINED_CRITERIA = list(itertools.combinations([\"Hard to Learn\", \"Ambiguous\", \"Easy to Learn\"], 2))\n",
    "RATIOS = [0.33, 0.5]\n",
    "CONVERGE_EPOCHS = [20]\n",
    "\n",
    "for RATIO in RATIOS:\n",
    "    for CONVERGE_EPOCH in CONVERGE_EPOCHS:\n",
    "        idx_dict, i2s = get_scores(OUTPUTS_PATH, CONVERGE_EPOCH, STRING_TRUNCATE)\n",
    "        df = calculate_statistics(CONVERGE_EPOCH, idx_dict, i2s)\n",
    "        for METRIC in METRICS:\n",
    "            merge_dfs = []\n",
    "            for CRITERION in CRITERIA:\n",
    "                idx_fname = create_ratio_fname(METRIC, CRITERION, CONVERGE_EPOCH, RATIO)\n",
    "                subset_df = choose_subset(df, METRIC, CRITERION, DATASET_NAME, idx_fname, ratio=RATIO)\n",
    "                merge_df = pd.merge(df, subset_df, on=[\"Index\", \"In\", \"Out\", \"In abbv.\", \"Out abbv.\", \"In Len\", \"Out Len\", \"In Rarity\", \"Out Rarity\", \\\n",
    "                                                       'Confidence - Inverse PPL', 'Variability - Inverse PPL', \\\n",
    "                                                        'Confidence - CHIA', 'Variability - CHIA', \\\n",
    "                                                        'Confidence - BLEU', 'Variability - BLEU'], indicator=f\"merge_{crit2abv[CRITERION]}\", how='outer')\n",
    "                # assert len(df) == len(merge_df), f\"Original and merged dataset sizes do not match!: Original size: {len(df)}, Merged size: {len(merge_df)}\"\n",
    "                # plot(merge_df, plot_type=\"inv_ppl\")\n",
    "                merge_dfs.append(merge_df)\n",
    "                # desc_df = subset_df.describe()\n",
    "                # print(f\"{METRIC} - {CRITERION}: \", f'In Len: {desc_df[\"In Len\"][1]:.2f}, Out Len: {desc_df[\"Out Len\"][1]:.2f}, In Rarity: {desc_df[\"In Rarity\"][1]:.2f}, Out Rarity: {desc_df[\"Out Rarity\"][1]:.2f}')\n",
    "\n",
    "            merge_df = merge_dfs[0]\n",
    "            for i in range(1, len(merge_dfs)):\n",
    "                merge_df = pd.merge(merge_df, merge_dfs[i], on=[\"Index\", \"In\", \"Out\", \"In abbv.\", \"Out abbv.\", \"In Len\", \"Out Len\", \"In Rarity\", \"Out Rarity\", \\\n",
    "                                                        'Confidence - Inverse PPL', 'Variability - Inverse PPL', \\\n",
    "                                                        'Confidence - CHIA', 'Variability - CHIA', \\\n",
    "                                                        'Confidence - BLEU', 'Variability - BLEU'], how='outer')\n",
    "                #merge_subset_df[\"_merge\"] = merge_subset_df[\"_merge\"].cat.remove_categories(\"right_only\")\n",
    "                print(merge_df.columns)\n",
    "\n",
    "            merge_df[\"combined\"] = merge_df[\"merge_ambiguous\"].astype(str) + merge_df[\"merge_easy_to_learn\"].astype(str) + merge_df[\"merge_hard_to_learn\"].astype(str)\n",
    "            plot(merge_df, plot_type=\"inv_ppl\", color_column=\"combined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78eecfc-ee35-49bd-9694-12fd04eb64a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = \"cfq\"\n",
    "OUTPUTS_PATH = outputs_path(DATASET_NAME)\n",
    "METRICS = [\"BLEU\"] #[\"Inverse PPL\", \"CHIA\"]\n",
    "CRITERIA = [\"Hard to Learn\", \"Ambiguous\", \"Easy to Learn\"] #, \"Ambiguous\",  \"Random\"]\n",
    "COMBINED_CRITERIA = list(itertools.combinations([\"Hard to Learn\", \"Ambiguous\", \"Easy to Learn\"], 2))\n",
    "RATIOS = [0.5]\n",
    "CONVERGE_EPOCHS = [20]\n",
    "\n",
    "for RATIO in RATIOS:\n",
    "    for CONVERGE_EPOCH in CONVERGE_EPOCHS:\n",
    "        idx_dict, i2s = get_scores(OUTPUTS_PATH, CONVERGE_EPOCH, STRING_TRUNCATE, min_epoch=3)\n",
    "        df = calculate_statistics(CONVERGE_EPOCH, idx_dict, i2s)\n",
    "        for METRIC in METRICS:\n",
    "            for CRITERION in CRITERIA:\n",
    "                idx_fname = create_ratio_fname(METRIC, CRITERION, CONVERGE_EPOCH, RATIO)\n",
    "                subset_df = choose_subset(df, METRIC, CRITERION, DATASET_NAME, idx_fname, ratio=RATIO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf6b209-293d-450f-9ad1-3cfacef2afb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_df[\"combined\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420f4708-cc74-4a4f-85d5-6b9f25d01da4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eeddcebe-c42a-478a-8a46-01403d42d9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = \"42/cogs\"\n",
    "OUTPUTS_PATH = outputs_path(DATASET_NAME)\n",
    "\n",
    "METRICS = [\"Inverse PPL\", \"CHIA\", \"BLEU\"]\n",
    "CRITERIA = [\"Hard to Learn\", \"Easy to Learn\", \"Ambiguous\"]\n",
    "COMBINED_CRITERIA = list(itertools.combinations([\"Hard to Learn\", \"Ambiguous\", \"Easy to Learn\"], 2))\n",
    "CONVERGE_EPOCH = 10\n",
    "\n",
    "for METRIC in METRICS:\n",
    "    for CRITERIA in COMBINED_CRITERIA:\n",
    "        subset_dfs = []\n",
    "        for CRITERION in CRITERIA:\n",
    "            idx_dict, i2s = get_scores(OUTPUTS_PATH, CONVERGE_EPOCH, STRING_TRUNCATE)\n",
    "            df = calculate_statistics(CONVERGE_EPOCH, idx_dict, i2s)\n",
    "            idx_fname = create_fname(METRIC, CRITERION, CONVERGE_EPOCH)\n",
    "            subset_df = choose_subset(df, METRIC, CRITERION, DATASET_NAME, idx_fname, write=False)\n",
    "            subset_dfs.append(subset_df)\n",
    "        idx_fname = create_comb_fname(METRIC, CRITERIA[0], CRITERIA[1], CONVERGE_EPOCH)\n",
    "        combined_set_df = combine_subsets(df, subset_dfs, DATASET_NAME, idx_fname)\n",
    "        \n",
    "        print(len(combined_set_df) / len(df))\n",
    "        desc_df = subset_df.describe()\n",
    "        #print(METRIC, CRITERION, f'In Len Mean: {desc_df[\"In Len\"][1]}', f'Out Len Mean: {desc_df[\"Out Len\"][1]}', f'In Rar Mean: {desc_df[\"In Rarity\"][1]}', f'Out Rar Mean: {desc_df[\"Out Rarity\"][1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bcdb2b80-aad7-4385-8967-13308bc0196c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inv_ppl_hard_to_learn_ambiguous_10.pickle 12078\n",
      "inv_ppl_hard_to_learn_easy_to_learn_10.pickle 12077\n",
      "inv_ppl_ambiguous_easy_to_learn_10.pickle 12077\n",
      "chia_hard_to_learn_ambiguous_10.pickle 12078\n",
      "chia_hard_to_learn_easy_to_learn_10.pickle 12077\n",
      "chia_ambiguous_easy_to_learn_10.pickle 12077\n",
      "bleu_hard_to_learn_ambiguous_10.pickle 12078\n",
      "bleu_hard_to_learn_easy_to_learn_10.pickle 12077\n",
      "bleu_ambiguous_easy_to_learn_10.pickle 12077\n"
     ]
    }
   ],
   "source": [
    "DATASET_NAME = \"42/cogs\"\n",
    "OUTPUTS_PATH = outputs_path(DATASET_NAME)\n",
    "\n",
    "METRICS = [\"Inverse PPL\", \"CHIA\", \"BLEU\"]\n",
    "CRITERIA = [\"Hard to Learn\", \"Easy to Learn\", \"Ambiguous\"]\n",
    "COMBINED_CRITERIA = list(itertools.combinations([\"Hard to Learn\", \"Ambiguous\", \"Easy to Learn\"], 2))\n",
    "CONVERGE_EPOCH = 10\n",
    "\n",
    "for METRIC in METRICS:\n",
    "    for CRITERIA in COMBINED_CRITERIA:\n",
    "            idx_fname = create_comb_fname(METRIC, CRITERIA[0], CRITERIA[1], CONVERGE_EPOCH)\n",
    "            print(idx_fname, len(list(read_pickle(os.path.join(\"subsets\", DATASET_NAME, idx_fname)))))\n",
    "\n",
    "\n",
    "# for METRIC in METRICS:\n",
    "#    for CRITERION in CRITERIA:\n",
    "#        for RATIO in RATIOS:\n",
    "#            idx_fname = create_ratio_fname(METRIC, CRITERION, CONVERGE_EPOCH, RATIO)\n",
    "#            print(idx_fname, len(list(read_pickle(os.path.join(\"subsets\", DATASET_NAME, idx_fname)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "08e52321-9500-4891-b3f0-6e79d108a467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start subset 31595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64148/64148 [00:02<00:00, 22948.48it/s]\n",
      "  0%|          | 0/31595 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n",
      "0 0\n",
      "start subset 31595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64148/64148 [00:02<00:00, 22210.00it/s]\n",
      "  0%|          | 7/31595 [00:00<00:05, 5681.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 7\n",
      "1 7\n",
      "2 7\n",
      "3 7\n",
      "4 7\n",
      "5 7\n",
      "6 7\n",
      "7 7\n",
      "7 7\n",
      "subset_idx: 47871\n",
      "0.4999947776860972\n",
      "start subset 31595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64148/64148 [00:02<00:00, 22664.66it/s]\n",
      "  0%|          | 0/31595 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n",
      "0 0\n",
      "start subset 31595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64148/64148 [00:02<00:00, 21621.29it/s]\n",
      "  0%|          | 10/31595 [00:00<00:05, 5838.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 10\n",
      "1 10\n",
      "2 10\n",
      "3 10\n",
      "4 10\n",
      "5 10\n",
      "6 10\n",
      "7 10\n",
      "8 10\n",
      "9 10\n",
      "10 10\n",
      "10 10\n",
      "subset_idx: 47871\n",
      "0.4999947776860972\n",
      "start subset 31595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64148/64148 [00:02<00:00, 21728.13it/s]\n",
      "  0%|          | 7/31595 [00:00<00:05, 5346.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 7\n",
      "1 7\n",
      "2 7\n",
      "3 7\n",
      "4 7\n",
      "5 7\n",
      "6 7\n",
      "7 7\n",
      "7 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/kuacc/users/oince22/.conda/envs/plotly-2/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3457, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_440432/1737004244.py\", line 13, in <module>\n",
      "    idx_dict, i2s = get_scores(OUTPUTS_PATH, CONVERGE_EPOCH, STRING_TRUNCATE)\n",
      "  File \"/tmp/ipykernel_440432/884308723.py\", line 51, in get_scores\n",
      "    in_rarity, out_rarity = get_rarity(v[\"in\"], v[\"out\"], in_v, out_v)\n",
      "  File \"/tmp/ipykernel_440432/1708361665.py\", line 18, in get_rarity\n",
      "    return -np.log(in_rarity), -np.log(out_rarity)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/kuacc/users/oince22/.conda/envs/plotly-2/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2077, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/kuacc/users/oince22/.conda/envs/plotly-2/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/kuacc/users/oince22/.conda/envs/plotly-2/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/kuacc/users/oince22/.conda/envs/plotly-2/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/kuacc/users/oince22/.conda/envs/plotly-2/lib/python3.7/inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/kuacc/users/oince22/.conda/envs/plotly-2/lib/python3.7/inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/kuacc/users/oince22/.conda/envs/plotly-2/lib/python3.7/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/kuacc/users/oince22/.conda/envs/plotly-2/lib/python3.7/inspect.py\", line 739, in getmodule\n",
      "    f = getabsfile(module)\n",
      "  File \"/kuacc/users/oince22/.conda/envs/plotly-2/lib/python3.7/inspect.py\", line 708, in getabsfile\n",
      "    _filename = getsourcefile(object) or getfile(object)\n",
      "  File \"/kuacc/users/oince22/.conda/envs/plotly-2/lib/python3.7/inspect.py\", line 693, in getsourcefile\n",
      "    if os.path.exists(filename):\n",
      "  File \"/kuacc/users/oince22/.conda/envs/plotly-2/lib/python3.7/genericpath.py\", line 19, in exists\n",
      "    os.stat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_440432/1737004244.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mCRITERION\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mCRITERIA\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             \u001b[0midx_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi2s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOUTPUTS_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCONVERGE_EPOCH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSTRING_TRUNCATE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m             \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_statistics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCONVERGE_EPOCH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi2s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_440432/884308723.py\u001b[0m in \u001b[0;36mget_scores\u001b[0;34m(dir_path, converge_epoch, string_truncate, min_epoch)\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0midx_to_sentences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0min_rarity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_rarity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_rarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"in\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"out\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_v\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0mi2s\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"In Rarity\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_rarity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_440432/1708361665.py\u001b[0m in \u001b[0;36mget_rarity\u001b[0;34m(in_txt, out_txt, in_v, out_v)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_rarity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_rarity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/.conda/envs/plotly-2/lib/python3.7/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2076\u001b[0m                         \u001b[0;31m# in the engines. This should return a list of strings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2077\u001b[0;31m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2078\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'KeyboardInterrupt' object has no attribute '_render_traceback_'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/plotly-2/lib/python3.7/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2078\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2079\u001b[0m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[0;32m-> 2080\u001b[0;31m                                             value, tb, tb_offset=tb_offset)\n\u001b[0m\u001b[1;32m   2081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2082\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_showtraceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/plotly-2/lib/python3.7/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1366\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m         return FormattedTB.structured_traceback(\n\u001b[0;32m-> 1368\u001b[0;31m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[0m\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/plotly-2/lib/python3.7/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1266\u001b[0m             \u001b[0;31m# Verbose modes need a full traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1267\u001b[0m             return VerboseTB.structured_traceback(\n\u001b[0;32m-> 1268\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1269\u001b[0m             )\n\u001b[1;32m   1270\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'Minimal'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/plotly-2/lib/python3.7/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0;32m-> 1125\u001b[0;31m                                                                tb_offset)\n\u001b[0m\u001b[1;32m   1126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m         \u001b[0mcolors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mColors\u001b[0m  \u001b[0;31m# just a shorthand + quicker name lookup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/plotly-2/lib/python3.7/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[0;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/plotly-2/lib/python3.7/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[0;34m(etype, value, records)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[0;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "DATASET_NAME = \"42/cfq\"\n",
    "OUTPUTS_PATH = outputs_path(DATASET_NAME)\n",
    "\n",
    "METRICS = [\"Inverse PPL\", \"CHIA\", \"BLEU\"]\n",
    "CRITERIA = [\"Hard to Learn\", \"Easy to Learn\", \"Ambiguous\"]\n",
    "COMBINED_CRITERIA = list(itertools.combinations([\"Hard to Learn\", \"Ambiguous\", \"Easy to Learn\"], 2))\n",
    "CONVERGE_EPOCH = 20\n",
    "\n",
    "for METRIC in METRICS:\n",
    "    for CRITERIA in COMBINED_CRITERIA:\n",
    "        subset_dfs = []\n",
    "        for CRITERION in CRITERIA:\n",
    "            idx_dict, i2s = get_scores(OUTPUTS_PATH, CONVERGE_EPOCH, STRING_TRUNCATE)\n",
    "            df = calculate_statistics(CONVERGE_EPOCH, idx_dict, i2s)\n",
    "            idx_fname = create_fname(METRIC, CRITERION, CONVERGE_EPOCH)\n",
    "            subset_df = choose_subset(df, METRIC, CRITERION, DATASET_NAME, idx_fname, write=False)\n",
    "            subset_dfs.append(subset_df)\n",
    "        idx_fname = create_comb_fname(METRIC, CRITERIA[0], CRITERIA[1], CONVERGE_EPOCH)\n",
    "        combined_set_df = combine_subsets(df, subset_dfs, DATASET_NAME, idx_fname)\n",
    "        \n",
    "        print(len(combined_set_df) / len(df))\n",
    "        desc_df = subset_df.describe()\n",
    "        #print(METRIC, CRITERION, f'In Len Mean: {desc_df[\"In Len\"][1]}', f'Out Len Mean: {desc_df[\"Out Len\"][1]}', f'In Rar Mean: {desc_df[\"In Rarity\"][1]}', f'Out Rar Mean: {desc_df[\"Out Rarity\"][1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa27fba-faa1-484f-b07c-9a0db185e734",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa9d4c8-eec4-4d68-ad2e-9111397c6953",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b72b236-10ee-4298-a123-dce854744882",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = \"scan_length\"\n",
    "OUTPUTS_PATH = outputs_path(DATASET_NAME)\n",
    "METRIC = \"CHIA\"\n",
    "CRITERIA = \"Hard to Learn\"\n",
    "CONVERGE_EPOCH = 30\n",
    "\n",
    "idx_dict, i2s = get_scores(OUTPUTS_PATH, CONVERGE_EPOCH, STRING_TRUNCATE)\n",
    "df = calculate_statistics(CONVERGE_EPOCH, idx_dict, i2s)\n",
    "idx_fname = create_fname(METRIC, CRITERIA, CONVERGE_EPOCH)\n",
    "subset_df = choose_subset(df, METRIC, CRITERIA, DATASET_NAME, idx_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b84eee1-dea9-4283-9d3a-aca5858b5cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fecc0c5-917c-4bc7-82e8-fc9960a8facc",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5290f9-8645-4c19-ab8a-a4807b93f3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = \"scan_jump\"\n",
    "OUTPUTS_PATH = outputs_path(DATASET_NAME)\n",
    "METRIC = \"CHIA\"\n",
    "CRITERIA = \"Hard to Learn\"\n",
    "CONVERGE_EPOCH = 30\n",
    "\n",
    "idx_dict, i2s = get_scores(OUTPUTS_PATH, CONVERGE_EPOCH, STRING_TRUNCATE)\n",
    "df = calculate_statistics(CONVERGE_EPOCH, idx_dict, i2s)\n",
    "idx_fname = create_fname(METRIC, CRITERIA, CONVERGE_EPOCH)\n",
    "subset_df = choose_subset(df, METRIC, CRITERIA, DATASET_NAME, idx_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db220853-5e76-4f14-a28d-ce15f987b8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5761e02a-6200-4e0e-9aa7-0dcb4652c6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d139b1c8-6c44-48a2-aa6e-936c7a156a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0137f9-7d90-4afb-a054-de2cc56e0324",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = \"pcfg\"\n",
    "OUTPUTS_PATH = outputs_path(DATASET_NAME)\n",
    "METRIC = \"CHIA\"\n",
    "CRITERIA = \"Hard to Learn\"\n",
    "CONVERGE_EPOCH = 140\n",
    "\n",
    "idx_dict, i2s = get_scores(OUTPUTS_PATH, CONVERGE_EPOCH, STRING_TRUNCATE)\n",
    "df = calculate_statistics(CONVERGE_EPOCH, idx_dict, i2s)\n",
    "idx_fname = create_fname(METRIC, CRITERIA, CONVERGE_EPOCH)\n",
    "subset_df = choose_subset(df, METRIC, CRITERIA, DATASET_NAME, idx_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651b6c1b-9b5e-4cdf-b88c-d86b79dbf281",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56b342e-9e75-42d5-b608-656081535685",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bf9bfb-32c1-468e-9f15-60493c553ec8",
   "metadata": {},
   "source": [
    "subset_df_in = set(subset_df[\"In\"].tolist())\n",
    "subset_df_out = set(subset_df[\"Out\"].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0553192c-6d24-4ad5-9b8f-6b39117d8bb1",
   "metadata": {},
   "source": [
    "subset_pkl = read_pickle(\"../scores/cogs/idx_to_sentences.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87a7e7f-5c17-4ec5-8be5-8d01353590ef",
   "metadata": {},
   "source": [
    "subset_pkl_in = []\n",
    "subset_pkl_out = []\n",
    "\n",
    "for i, text in subset_pkl.items():\n",
    "    subset_pkl_in.append(text[\"in\"])\n",
    "    subset_pkl_out.append(text[\"out\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab763559-2449-410a-960b-d999ebc0e234",
   "metadata": {},
   "source": [
    "subset_pkl_in = set(subset_pkl_in)\n",
    "subset_pkl_out = set(subset_pkl_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23edd506-124d-4657-ba1e-906fb731f680",
   "metadata": {},
   "source": [
    "subset_df_in - subset_pkl_in, len(subset_df_in), len(subset_pkl_in)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
